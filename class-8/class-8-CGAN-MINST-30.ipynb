{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nispoe/GWU-SEAS-8525/blob/main/class-8/class-8-CGAN-MINST-30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcWIDZifSgqI"
      },
      "source": [
        "**1. Import Libraries and Load the MNIST Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJxvtiifSogu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (_, _) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
        "x_train = (x_train - 127.5) / 127.5\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).batch(256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsULIWX5Vytq"
      },
      "source": [
        "**2. Build the Generator and Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2NGY9CfT09v"
      },
      "outputs": [],
      "source": [
        "def build_generator(latent_dim, num_classes=10):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(latent_dim + num_classes,)),\n",
        "        layers.Dense(7*7*256, use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Reshape((7, 7, 256)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=5, strides=1, padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding='same', use_bias=False, activation='tanh'),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_discriminator(image_shape):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=image_shape),  # Explicit Input layer\n",
        "        layers.Conv2D(64, kernel_size=5, strides=2, padding='same'),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Conv2D(128, kernel_size=5, strides=2, padding='same'),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1),\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dquX6iurWVJe"
      },
      "source": [
        "**3. cGAN model with gradient penalty**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B174YEXGWVoT"
      },
      "outputs": [],
      "source": [
        "class CGAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim, num_classes):\n",
        "        super(CGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(CGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        real_images, labels = data\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        labels = tf.cast(labels, tf.float32)\n",
        "        random_latent_vectors = tf.concat([noise, labels], axis=1)\n",
        "        generated_images = self.generator(random_latent_vectors, training=True)\n",
        "\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "            real_output = self.discriminator(real_images, training=True)\n",
        "            fake_output = self.discriminator(generated_images, training=True)\n",
        "            d_loss_real = self.loss_fn(tf.ones_like(real_output), real_output)\n",
        "            d_loss_fake = self.loss_fn(tf.zeros_like(fake_output), fake_output)\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "        grads_disc = disc_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_optimizer.apply_gradients(zip(grads_disc, self.discriminator.trainable_variables))\n",
        "\n",
        "        noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        labels = tf.cast(labels, tf.float32)\n",
        "        random_latent_vectors = tf.concat([noise, labels], axis=1)\n",
        "\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            generated_images = self.generator(random_latent_vectors, training=True)\n",
        "            gen_img_labels = self.discriminator(generated_images, training=True)\n",
        "            g_loss = self.loss_fn(tf.ones_like(gen_img_labels), gen_img_labels)\n",
        "\n",
        "        grads_gen = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        self.g_optimizer.apply_gradients(zip(grads_gen, self.generator.trainable_variables))\n",
        "\n",
        "        return {'d_loss': d_loss, 'g_loss': g_loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkbWzfXiUQOF"
      },
      "source": [
        "**4. Compile and train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr3UN2ZTUYRe",
        "outputId": "ea353e00-eb30-471c-abac-40c5bf71f1ce"
      },
      "outputs": [],
      "source": [
        "latent_dim = 100\n",
        "num_classes = 10\n",
        "generator = build_generator(latent_dim, num_classes)\n",
        "discriminator = build_discriminator((28, 28, 1))\n",
        "\n",
        "cgan = CGAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim, num_classes=num_classes)\n",
        "cgan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(1e-4),\n",
        "    g_optimizer=keras.optimizers.Adam(1e-4),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        ")\n",
        "\n",
        "cgan.fit(train_dataset, epochs=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**5. Display generated images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "Ffq1tXb_WreO",
        "outputId": "fcdde483-ce2b-422e-82dc-2053ae495873"
      },
      "outputs": [],
      "source": [
        "def display_generated_images(generator, num_examples=10, latent_dim=100, num_classes=10):\n",
        "    plt.figure(figsize=(30, 10))\n",
        "    for digit in range(num_classes):\n",
        "        noise = tf.random.normal([num_examples, latent_dim])\n",
        "        digit_labels = tf.one_hot([digit] * num_examples, depth=num_classes)\n",
        "        generator_input = tf.concat([noise, digit_labels], axis=1)\n",
        "\n",
        "        generated_images = generator(generator_input, training=False)\n",
        "\n",
        "        generated_images = tf.sigmoid(generated_images).numpy()\n",
        "\n",
        "        generated_images = np.clip(generated_images * 255, 0, 255).astype('uint8')\n",
        "\n",
        "        for i in range(num_examples):\n",
        "            ax = plt.subplot(num_classes, num_examples, digit * num_examples + i + 1)\n",
        "            plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "display_generated_images(generator, num_examples=10, latent_dim=100, num_classes=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
